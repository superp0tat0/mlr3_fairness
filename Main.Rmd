---
title: "Explore COMPAS"
output: pdf_document
author: Siyi Wei
---

```{r, include = F}
library(mlr3)
library(mlr3pipelines)
library(tidyverse)
library(caret)
library(reshape2)
```

## Explore COMPAS
We would like to explore the dataset COMPAS using mlr3 pipeline operations. I used the pre_processed COMPAS dataset from the fairness package.
Here I quote the introduction of COMPAS datasets "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a popular commercial algorithm used by judges and parole officers for scoring criminal defendantâ€™s likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants, and against black inmates, based on a 2 year follow up study (i.e who actually committed crimes or violent crimes after 2 years). The pattern of mistakes, as measured by precision/sensitivity is notable."
In the following study we want to explore whether the algorithm will give a biased prediction

```{r, include = F}
library(fairness)
data("compas")
head(compas)
```

* We could again use the classification Tree model to predict the Two Year Recidivism. The results are quite similar to the raw scores from COMPAS algorithm. There is a 33.68% Classification error and moreover, The False Positive is 186 and False Negative is 230. FN is slightly higher.
```{r}
sample_id = sample(c(1:6172), 0.8*6172)
compas_train = compas[sample_id,-c(8,9)]
compas_test = compas[-sample_id,-c(8,9)]
rownames(compas_test) = seq_len(nrow(compas_test))

compas_train_backend = as_data_backend(compas_train)
compas_test_backend = as_data_backend(compas_test)

black_id = as.numeric(rownames(compas_test[compas_train$ethnicity == "African_American",]))
black_id2 = as.numeric(rownames(compas_test[compas_test$ethnicity == "African_American",]))
compas_black_backend = as_data_backend(compas_train[black_id,])
compas_black_backend2 = as_data_backend(compas_test[black_id2,])

task_train_COMPAS <- TaskClassif$new(id = "train", backend = compas_train_backend, target = "Two_yr_Recidivism")
task_test_COMPAS <- TaskClassif$new(id = "test", backend = compas_test_backend, target = "Two_yr_Recidivism")

task_train_COMPAS_black <- TaskClassif$new(id = "test", backend = compas_black_backend, target = "Two_yr_Recidivism")
task_test_COMPAS_black <- TaskClassif$new(id = "test", backend = compas_black_backend2, target = "Two_yr_Recidivism")

learner = lrn("classif.rpart", cp = .01)
learner$train(task_train_COMPAS_black)
prediction <- learner$predict(task_test_COMPAS)
confusionMatrix(prediction$response, prediction$truth)

learner_black = lrn("classif.rpart", cp = .01)
learner_black$train(task_train_COMPAS_black)
prediction_black <- learner_black$predict(task_test_COMPAS_black)
confusionMatrix(prediction_black$response, prediction_black$truth)
```
* We want to ask the next question then. Among those wrong predictions. Will ethnicity or gender play a role?
* From gender. We could see we have a quite imbalanced dataset where male is more than female. This could potentially be a problem. Other than that, the prediction error for male is 34.33% and the prediction error for females is 30.96%. The prediction on females is more accurate than male.
* The False Positive rate for male is 32.92%. The False Negative rate is 35.72%. So the FN rate for male is higher than the FP rate. For females, the FP rate is 14.37% and the FN rate is 64.55%. We could see the algorithm is biased on gender. 
```{r}
#The male and female test datasets
male_id = as.numeric(rownames(compas_test[compas_test$Female == "Male",]))
learner$train()
female_id = as.numeric(rownames(compas_test[compas_test$Female == "Female",]))

prediction_male <- learner$predict(task_test_COMPAS, row_ids = male_id)
prediction_female <- learner$predict(task_test_COMPAS, row_ids = female_id)

prediction_male$confusion
prediction_female$confusion
```

* For ethnicity. The Afican-American has prediction error to be 34% and the other ethnicity has prediction error to be 33.3%. Which is quite close
* However, we could see there is a significant difference in False Positive Rate and False Negative rate. For African-American, the FP rate is 41.23% and the FN rate is 27.48. For the other race, the FP rate is 17% and the FN rate is 57.14%. So the algorithm is biased in ethnicity.
```{r}
#The black and other race id.
black_id = as.numeric(rownames(compas_test[compas_test$ethnicity == "African_American",]))
other_id = as.numeric(rownames(compas_test[compas_test$ethnicity != "African_American",]))

prediction_black <- learner$predict(task_test_COMPAS, row_ids = black_id)
prediction_other <- learner$predict(task_test_COMPAS, row_ids = other_id)

prediction_black$confusion
prediction_other$confusion
```

* For different race subgroup. We could analyze their corresponding FP rates and FN rates. We could see Arican American is the only race group that has FN rates larger than FP rates. Maybe we could use hierarchical model to solve this problem? By fine tune the hyper parameters.
```{r}
race = unique(factor(compas$ethnicity))
store = data.frame(matrix(ncol=3))
colnames(store)<-c("FN", "FP")
compas$Two_yr_Recidivism <- as.factor(as.numeric(compas$Two_yr_Recidivism) - 1)
compas$predicted <- as.factor(compas$predicted)
for(r in race){
  current = compas[compas$ethnicity == r,]
  con <- confusionMatrix(current$predicted, current$Two_yr_Recidivism)
  store[r,1] = con$table[3]/(con$table[3] + con$table[4])
  store[r,2] = con$table[2]/(con$table[1] + con$table[2])
}


store <- store[-1,]
store$race = rownames(store)
store = melt(store)

ggplot(data=store, aes(x=race, y=value, fill=variable)) +
geom_bar(stat="identity", position=position_dodge())
```

```{r}
compas$Two_yr_Recidivism_01 <- ifelse(compas$Two_yr_Recidivism == 'yes', 1, 0)
equal_odds(data    = compas,
           outcome = 'Two_yr_Recidivism_01',
           probs   = 'probability',
           group   = 'ethnicity',
           cutoff  = 0.5,
           base    = 'Caucasian')
```

