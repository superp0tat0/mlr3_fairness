---
title: "Explore MNIST and COMPAS using mlr3"
output: pdf_document
author: Siyi Wei
---

```{r, include = F}
library(mlr3)
library(mlr3pipelines)
library(tidyverse)
library(caret)
```

# Google Summer of Code 2021.

## First Dataset: PCA with MNIST
The idea of this project is rather simple. In image recognition dataset MNIST is one of the most famous benchmark tests. However, most of the machine learning models do not perform well on it since the difficulty to extract the patterns. We could apply feature engineering to improve such problems. PCA is one common used trick on reducing dimension. Since we have 784 dimensions for the dataset. We could use PCA to reduce dimension, resolve the sparsity of the dataset and apply regularization at the same time.
In this project, we want to do a simple experiment. Since the data matrix is sparse. We want to apply PCA transformation on the dataset and compare the results with not apply PCA transformation.


## Experiment: Compare estimations from PCA and naive model
* First we want to explore the performance of the naive classification algorithm on MNIST datasets. We could see even though the prediction is better than random guessing. But the classification errors are still large.
* We want to further explore the model performance using PCA, we first extract the 24 PCs from the datasets and use it as the features in the classification model. The model performance decreased. But we would like to know why by taking a closer look in the subgroup.
* For some of the sub-group (1,2,3) the model accuracy increases significantly. Some of them stay unchanged and some of them decrease significantly. For example, after the PCA transformation. The model is more likely to classify 9 as 4 or 2 as 6.
* We could potentially resolve this issue by increasing the number of PCs. However it could not perform significantly better than the original dataset since the tree model we have chosen.

```{r}
#Create the Data Backend
sample_id = sample(c(1:10000), 0.8*10000)
dataset_mnist <- read_csv("./MNIST/train.csv")[1:10000,]
dataset_mnist$label = as.factor(dataset_mnist$label)

train <- dataset_mnist[sample_id,]
train_backend <- as_data_backend(train)
test <- dataset_mnist[-sample_id,]
test_backend <- as_data_backend(test)

#Create the Task
task_train_MNIST <- TaskClassif$new(id = "train", backend = train, target = "label")
task_test_MNIST <- TaskClassif$new(id = "test", backend = test, target = "label")

#Create the pipeline operations
scale_po = po("scale")
pca_po = mlr_pipeops$get("pca", param_vals = list(rank. = 24))
learner_po =  mlr_pipeops$get("learner", mlr_learners$get("classif.rpart"))

# Create the model
pca_learner = scale_po %>>% pca_po %>>% learner_po
learner = scale_po %>>% learner_po
```


```{r}
# Train the naive classification tree model
learner$train(task_train_MNIST)
prediction <- learner$predict(task_test_MNIST)

# evaluate the performance
sum(prediction$classif.rpart.output$truth == prediction$classif.rpart.output$response)/2000

confusionMatrix(prediction$classif.rpart.output$truth, prediction$classif.rpart.output$response)$table
```


```{r}
#Train the PCA classification tree model and evaluate the performance
pca_learner$train(task_train_MNIST)
prediction2 <- pca_learner$predict(task_test_MNIST)

sum(prediction2$classif.rpart.output$truth == prediction2$classif.rpart.output$response)/2000

confusionMatrix(prediction2$classif.rpart.output$truth, prediction2$classif.rpart.output$response)$table
```

## Second Dataset: Explore COMPAS

For the Second part. We would like to explore the dataset COMPAS using mlr3 pipeline operations. I used the pre_processed COMPAS dataset from the fairness package.
Here I quote the introduction of COMPAS datasets "COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a popular commercial algorithm used by judges and parole officers for scoring criminal defendantâ€™s likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants, and against black inmates, based on a 2 year follow up study (i.e who actually committed crimes or violent crimes after 2 years). The pattern of mistakes, as measured by precision/sensitivity is notable."
In the following study we want to explore whether the algorithm will give a biased prediction

```{r, include = F}
library(fairness)
data("compas")
head(compas)
```

* We could again use the classification Tree model to predict the Two Year Recidivism. There is a 33.68% Classification error and moreover, The False Positive is 186 and False Negative is 230. FN is slightly higher.
```{r}
sample_id = sample(c(1:6172), 0.8*6172)
compas_train = compas[sample_id,-c(8,9)]
compas_test = compas[-sample_id,-c(8,9)]
rownames(compas_test) = seq_len(nrow(compas_test))

compas_train_backend = as_data_backend(compas_train)
compas_test_backend = as_data_backend(compas_test)

task_train_COMPAS <- TaskClassif$new(id = "train", backend = compas_train_backend, target = "Two_yr_Recidivism")
task_test_COMPAS <- TaskClassif$new(id = "test", backend = compas_test_backend, target = "Two_yr_Recidivism")

learner = lrn("classif.rpart", cp = .01)
learner$train(task_train_COMPAS)
prediction <- learner$predict(task_test_COMPAS)

confusionMatrix(prediction$response, prediction$truth)
```
* We want to ask the next question then. Among those wrong predictions. Will ethnicity or gender play a role?
* From gender. We could see we have a quite imbalanced dataset where male is more than female. This could potentially be a problem. Other than that, the prediction error for male is 34.33% and the prediction error for females is 30.96%. The prediction on females is more accurate than male.
* The False Positive rate for male is 32.92%. The False Negative rate is 35.72%. So the FN rate for male is higher than the FP rate. For females, the FP rate is 14.37% and the FN rate is 64.55%. We could see the algorithm is biased on gender. 
```{r}
#The male and female test datasets
male_id = as.numeric(rownames(compas_test[compas_test$Female == "Male",]))
female_id = as.numeric(rownames(compas_test[compas_test$Female == "Female",]))

prediction_male <- learner$predict(task_test_COMPAS, row_ids = male_id)
prediction_female <- learner$predict(task_test_COMPAS, row_ids = female_id)

prediction_male$confusion
prediction_female$confusion
```

* For ethnicity. The Afican-American has prediction error to be 34% and the other ethnicity has prediction error to be 33.3%. Which is quite close
* However, we could see there is a significant difference in False Positive Rate and False Negative rate. For African-American, the FP rate is 41.23% and the FN rate is 27.48. For the other race, the FP rate is 17% and the FN rate is 57.14%. So the algorithm is biased in ethnicity.
```{r}
#The black and other race id.
black_id = as.numeric(rownames(compas_test[compas_test$ethnicity == "African_American",]))
other_id = as.numeric(rownames(compas_test[compas_test$ethnicity != "African_American",]))

prediction_black <- learner$predict(task_test_COMPAS, row_ids = black_id)
prediction_other <- learner$predict(task_test_COMPAS, row_ids = other_id)

prediction_black$confusion
prediction_other$confusion
```

```{r}
race = unique(factor(compas$ethnicity))
store = data.frame(matrix(ncol=2))
colnames(store)<-c("FN", "FP")
for(r in race){
  current = compas[compas$ethnicity == r,]
  con <- confusionMatrix(current$predicted, current$Two_yr_Recidivism)
  store[r,1] = con$table[3]/(con$table[3] + con$table[4])
  store[r,2] = con$table[2]/(con$table[1] + con$table[2])
}

store <- store[-1,]

ggplot(data=store2, aes(x=race, y=value, fill=variable)) +
geom_bar(stat="identity", position=position_dodge())
```

